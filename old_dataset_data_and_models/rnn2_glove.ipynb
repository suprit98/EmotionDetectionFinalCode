{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"rnn2_glove.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"SlLHEIxdyL6a","colab_type":"code","colab":{}},"source":["# Import libraries\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import gensim\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D\n","from keras.layers.embeddings import Embedding\n","from keras.callbacks import EarlyStopping\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from nltk.tokenize import WordPunctTokenizer\n","from collections import Counter\n","from string import punctuation, ascii_lowercase\n","import re\n","import joblib\n","from tqdm import tqdm\n","from gensim.models import Word2Vec\n","from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.layers.normalization import BatchNormalization\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, accuracy_score\n","from keras.metrics import accuracy,categorical_accuracy\n","from keras.initializers import Constant"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4I0YZ8JYyL6f","colab_type":"code","colab":{}},"source":["# prodata4 is preprocessed Data\n","# only run when want to train on data4\n","\n","\n","# df = pd.read_pickle(\"PreProcessed_data/prodata4.pkl\")\n","# df=df.rename(columns={\"label\":\"sentiment\"})\n","# print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"auaZgrnRyL6k","colab_type":"code","colab":{}},"source":["# prodata3 is preprocessed Data\n","# only run when want to train on data3\n","\n","\n","# df=pd.read_pickle(\"PreProcessed_data/prodata3.pkl\")\n","# print(df.sentiment.unique())\n","# print(df.sentiment.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKKnJ8uyyL6q","colab_type":"code","colab":{}},"source":["# To remove label surprise from prodata3\n","\n","\n","# df=df[df.sentiment!='surprise']\n","# print(df.sentiment.unique())\n","# print(df.sentiment.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9YXSHFigyL6s","colab_type":"code","colab":{}},"source":["# # tweet_data2_all.pkl is preprocessed Data2\n","\n","\n","# df=pd.read_pickle('PreProcessed_data/tweet_data2_all.pkl')\n","# df['content'] = [' '.join(map(str,l)) for l in df['content']]\n","\n","# # To remove following label from data2\n","\n","# df=df[df.sentiment!='neutral']\n","# df=df[df.sentiment!='love']\n","# df=df[df.sentiment!='surprise']\n","# print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STJW3Xk_yL6v","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpL677kOyL60","colab_type":"code","colab":{}},"source":["# Reading combined data of all above dataset after preprocessing\n","# only run when want to train on combined data\n","\n","#df=pd.read_pickle('PreProcessed_data/combineData.pkl')\n","#print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKz61Fv6yL65","colab_type":"code","colab":{},"outputId":"f4801960-708b-445c-baa8-05d537f88664"},"source":["# label value counts\n","\n","print(df.sentiment.value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["joy        6200\n","fear       6188\n","sadness    5695\n","anger      5206\n","Name: sentiment, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iPQMFIMPyL68","colab_type":"code","colab":{}},"source":["# local Preprocessing functions whih was not done previously\n","\n","replace_puncts = {'`': \"'\", '′': \"'\", '“':'\"', '”': '\"', '‘': \"'\"}\n","\n","strip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '•',  '~', '@', \n"," '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n"," '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n"," '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n"," '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n","\n","puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']\n","\n","def clean_str(x):\n","    x = str(x)\n","    \n","    x = x.lower()\n","    \n","    x = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n","    \n","    for k, v in replace_puncts.items():\n","        x = x.replace(k, f' {v} ')\n","        \n","    for punct in strip_chars:\n","        x = x.replace(punct, ' ') \n","    \n","    for punct in puncts:\n","        x = x.replace(punct, f' {punct} ')\n","        \n","    x = x.replace(\" '\", \" \")\n","    x = x.replace(\"' \", \" \")\n","        \n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJdtrTpGyL6_","colab_type":"code","colab":{}},"source":["# applying preprocessing\n","\n","df['content'] = df['content'].apply(clean_str)\n","print(df['content'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ljw9tP29yL7C","colab_type":"code","colab":{},"outputId":"f8e33169-9fb5-471f-caae-8f8385ce8e97"},"source":["print(df.sentiment.value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["joy        6200\n","fear       6188\n","sadness    5695\n","anger      5206\n","Name: sentiment, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t6TyV36WyL7H","colab_type":"code","colab":{},"outputId":"9f18b94c-8eaa-4a56-b594-260b318de126"},"source":["# data analysis for further use in model creation\n","\n","df['l'] = df['content'].apply(lambda x: len(str(x).split(' ')))\n","print(\"mean length of sentence: \" + str(df.l.mean()))\n","print(\"max length of sentence: \" + str(df.l.max()))\n","print(\"std dev length of sentence: \" + str(df.l.std()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean length of sentence: 13.326420198376915\n","max length of sentence: 136\n","std dev length of sentence: 9.966683924947404\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aiL5lNVfyL7K","colab_type":"code","colab":{}},"source":["# sequence length of tweet for training and prediction\n","\n","sequence_length = 136"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ij3TKiqzyL7M","colab_type":"code","colab":{},"outputId":"eaf68eae-5870-4aa4-929e-1e97f39b5766"},"source":["max_features = 20000 # this is the number of words we care about\n","\n","tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n","tokenizer.fit_on_texts(df['content'].values)\n","\n","# this takes our sentences and replaces each word with an integer\n","X = tokenizer.texts_to_sequences(df['content'].values)\n","\n","# we then pad the sequences so they're all the same length (sequence_length)\n","X = pad_sequences(X, sequence_length)\n","\n","y = pd.get_dummies(df['sentiment']).values\n","\n","# lets keep a couple of thousand samples back as a test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=41)\n","\n","print(\"test set size \" + str(len(X_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test set size 13974\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TmdEn3eGyL7P","colab_type":"code","colab":{},"outputId":"96c809b6-0209-432f-8fd6-73f9476c6405"},"source":["# word embedding from Glove\n","# download from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n","\n","embeddings_index = {}\n","f = open('../glove.twitter.27B/glove.twitter.27B.100d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1193515 word vectors.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t_kDokSbyL7T","colab_type":"code","colab":{},"outputId":"0fd94ad0-4aa8-4f5a-9be2-46804c70ff8b"},"source":["# unique tokens in combined data\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 19273 unique tokens.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pXpCVYPvyL7Y","colab_type":"code","colab":{},"outputId":"c15dc391-4ff0-43e5-a09d-2f9139355655"},"source":["num_words = min(max_features, len(word_index)) + 1\n","print(num_words)\n","\n","embedding_dim = 100\n","\n","# first create a matrix of zeros, this is our embedding matrix\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","# for each word in out tokenizer lets try to find that work in our w2v model\n","for word, i in word_index.items():\n","    if i > max_features:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # we found the word - add that words vector to the matrix\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # doesn't exist, assign a random vector\n","        embedding_matrix[i] = np.random.randn(embedding_dim)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["19274\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0_oSbiiIyL7a","colab_type":"code","colab":{},"outputId":"caada68d-5e32-4878-d511-528f969880f4"},"source":["# model creation\n","\n","model = Sequential()\n","model.add(Embedding(num_words,\n","                    embedding_dim,\n","                    embeddings_initializer=Constant(embedding_matrix),\n","                    input_length=sequence_length,\n","                    trainable=True))\n","model.add(SpatialDropout1D(0.2))\n","model.add(Bidirectional(LSTM(128, return_sequences=True)))\n","model.add(Dropout(0.2))\n","model.add(Bidirectional(LSTM(128)))\n","model.add(Dropout(0.2))\n","model.add(Dense(units=4, activation='sigmoid'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, 136, 100)          1927400   \n","_________________________________________________________________\n","spatial_dropout1d_6 (Spatial (None, 136, 100)          0         \n","_________________________________________________________________\n","bidirectional_11 (Bidirectio (None, 136, 256)          234496    \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 136, 256)          0         \n","_________________________________________________________________\n","bidirectional_12 (Bidirectio (None, 256)               394240    \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 4)                 1028      \n","=================================================================\n","Total params: 2,557,164\n","Trainable params: 2,557,164\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MCri65QbyL7f","colab_type":"code","colab":{},"outputId":"f4b25ea2-2823-4fd2-e67d-02f34db204f6"},"source":["# Train model\n","\n","batch_size = 128\n","\n","history = model.fit(X_train, y_train, epochs=6, batch_size=batch_size, verbose=1, validation_split=0.15)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/home/administrator/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 19795 samples, validate on 3494 samples\n","Epoch 1/6\n","19795/19795 [==============================] - 135s 7ms/step - loss: 1.1468 - accuracy: 0.4918 - val_loss: 1.3920 - val_accuracy: 0.4033\n","Epoch 2/6\n","19795/19795 [==============================] - 133s 7ms/step - loss: 0.9073 - accuracy: 0.6358 - val_loss: 1.3054 - val_accuracy: 0.4313\n","Epoch 3/6\n","19795/19795 [==============================] - 133s 7ms/step - loss: 0.7435 - accuracy: 0.7119 - val_loss: 1.1740 - val_accuracy: 0.5169\n","Epoch 4/6\n","19795/19795 [==============================] - 134s 7ms/step - loss: 0.6386 - accuracy: 0.7605 - val_loss: 0.9709 - val_accuracy: 0.6251\n","Epoch 5/6\n","19795/19795 [==============================] - 136s 7ms/step - loss: 0.5550 - accuracy: 0.7892 - val_loss: 1.1519 - val_accuracy: 0.5985\n","Epoch 6/6\n","19795/19795 [==============================] - 135s 7ms/step - loss: 0.4822 - accuracy: 0.8190 - val_loss: 1.1311 - val_accuracy: 0.6216\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MmUkM2wzyL7h","colab_type":"code","colab":{},"outputId":"74393463-d1ae-499c-c7a5-f77f68b69fc0"},"source":["print(X_test.shape,y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2593, 136) (2593, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zZHm7hbCyL7l","colab_type":"code","colab":{},"outputId":"6746f02b-ed83-4d67-ce40-a7de14f28699"},"source":["# load model from local disk if available else save from below code\n","\n","model = joblib.load('model_combinedata_83.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["c:\\users\\harsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hGkPkH3MyL7n","colab_type":"code","colab":{},"outputId":"6c1f5898-7a6b-4cd1-fc7f-a85b88da639d"},"source":["# loss and accuracy score\n","\n","model.evaluate(X_test, y_test, verbose=0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.4626077514435577, 0.8391298055648804]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"jxgZid4QyL7q","colab_type":"code","colab":{},"outputId":"927d9ec1-a982-4e76-b4da-2703f48c125d"},"source":["# Save the model as a pickle file on disk \n","joblib.dump(model, 'model_combinedata_83.pkl')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['model_combinedata_83.pkl']"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"code","metadata":{"id":"LX8E_R6-yL7s","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDgz7CLPyL7y","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHQYyCo7yL71","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qazRJQ4gyL74","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTI8upGEyL78","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZbsHRtdyL7-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVMuFFa0yL8A","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmvkFjjNyL8C","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBcdctRSyL8F","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9-muPpnyL8H","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGnBLelvyL8J","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tgXhVqpMyL8L","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6XU4UalyL8N","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oT9FilUqyL8P","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}