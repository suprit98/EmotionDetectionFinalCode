{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "import re\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.metrics import accuracy,categorical_accuracy\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prodata4 is preprocessed Data\n",
    "# only run when want to train on data4\n",
    "\n",
    "\n",
    "# df = pd.read_pickle(\"PreProcessed_data/prodata4.pkl\")\n",
    "# df=df.rename(columns={\"label\":\"sentiment\"})\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prodata3 is preprocessed Data\n",
    "# only run when want to train on data3\n",
    "\n",
    "\n",
    "# df=pd.read_pickle(\"PreProcessed_data/prodata3.pkl\")\n",
    "# print(df.sentiment.unique())\n",
    "# print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove label surprise from prodata3\n",
    "\n",
    "\n",
    "# df=df[df.sentiment!='surprise']\n",
    "# print(df.sentiment.unique())\n",
    "# print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tweet_data2_all.pkl is preprocessed Data2\n",
    "\n",
    "\n",
    "# df=pd.read_pickle('PreProcessed_data/tweet_data2_all.pkl')\n",
    "# df['content'] = [' '.join(map(str,l)) for l in df['content']]\n",
    "\n",
    "# # To remove following label from data2\n",
    "\n",
    "# df=df[df.sentiment!='neutral']\n",
    "# df=df[df.sentiment!='love']\n",
    "# df=df[df.sentiment!='surprise']\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content sentiment\n",
      "1646   how fuk who heck move fridg I knock landlord d...     anger\n",
      "1647   significant oth indian uber driver call someon...     anger\n",
      "1648   I ask parcel deliv pick store address fume poo...     anger\n",
      "1649   ef whichev butt wipe pull financial independen...     anger\n",
      "1650   dont join put phone you talk rude take money a...     anger\n",
      "...                                                  ...       ...\n",
      "7228   When I learnt that I had been admitted to the ...       joy\n",
      "10306                        football training going wet       joy\n",
      "8107                    i love writing receiving letters       joy\n",
      "4225          been sex people brave put business  saying       joy\n",
      "21003  dawned upon world promised creation god salvat...       joy\n",
      "\n",
      "[23289 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reading combined data of all above dataset after preprocessing\n",
    "\n",
    "df=pd.read_pickle('PreProcessed_data/combineData.pkl')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        6200\n",
      "fear       6188\n",
      "sadness    5695\n",
      "anger      5206\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# label value counts\n",
    "\n",
    "print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local Preprocessing functions whih was not done previously\n",
    "\n",
    "replace_puncts = {'`': \"'\", '′': \"'\", '“':'\"', '”': '\"', '‘': \"'\"}\n",
    "\n",
    "strip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '•',  '~', '@', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']\n",
    "\n",
    "def clean_str(x):\n",
    "    x = str(x)\n",
    "    \n",
    "    x = x.lower()\n",
    "    \n",
    "    x = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n",
    "    \n",
    "    for k, v in replace_puncts.items():\n",
    "        x = x.replace(k, f' {v} ')\n",
    "        \n",
    "    for punct in strip_chars:\n",
    "        x = x.replace(punct, ' ') \n",
    "    \n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "        \n",
    "    x = x.replace(\" '\", \" \")\n",
    "    x = x.replace(\"' \", \" \")\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646     how fuk who heck move fridg i knock landlord d...\n",
      "1647     significant oth indian uber driver call someon...\n",
      "1648     i ask parcel deliv pick store address fume poo...\n",
      "1649     ef whichev butt wipe pull financial independen...\n",
      "1650     dont join put phone you talk rude take money a...\n",
      "                               ...                        \n",
      "7228     when i learnt that i had been admitted to the ...\n",
      "10306                          football training going wet\n",
      "8107                      i love writing receiving letters\n",
      "4225            been sex people brave put business  saying\n",
      "21003    dawned upon world promised creation god salvat...\n",
      "Name: content, Length: 23289, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# applying preprocessing\n",
    "\n",
    "df['content'] = df['content'].apply(clean_str)\n",
    "print(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy        6200\n",
      "fear       6188\n",
      "sadness    5695\n",
      "anger      5206\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of sentence: 13.326420198376915\n",
      "max length of sentence: 136\n",
      "std dev length of sentence: 9.966683924947404\n"
     ]
    }
   ],
   "source": [
    "# data analysis for further use in model creation\n",
    "\n",
    "df['l'] = df['content'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(df.l.mean()))\n",
    "print(\"max length of sentence: \" + str(df.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(df.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length of tweet for training and prediction\n",
    "\n",
    "sequence_length = 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size 13974\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000 # this is the number of words we care about\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(df['content'].values)\n",
    "\n",
    "# this takes our sentences and replaces each word with an integer\n",
    "X = tokenizer.texts_to_sequences(df['content'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "y = pd.get_dummies(df['sentiment']).values\n",
    "\n",
    "# lets keep a couple of thousand samples back as a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=41)\n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193515 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# word embedding from Glove\n",
    "# download from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../glove.twitter.27B/glove.twitter.27B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19273 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# unique tokens in combined data\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19274\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 136, 100)          1927400   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 136, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 136, 256)          234496    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 136, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 2,557,164\n",
      "Trainable params: 2,557,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model creation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=sequence_length,\n",
    "                    trainable=True))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4, activation='sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19795 samples, validate on 3494 samples\n",
      "Epoch 1/6\n",
      "19795/19795 [==============================] - 135s 7ms/step - loss: 1.1468 - accuracy: 0.4918 - val_loss: 1.3920 - val_accuracy: 0.4033\n",
      "Epoch 2/6\n",
      "19795/19795 [==============================] - 133s 7ms/step - loss: 0.9073 - accuracy: 0.6358 - val_loss: 1.3054 - val_accuracy: 0.4313\n",
      "Epoch 3/6\n",
      "19795/19795 [==============================] - 133s 7ms/step - loss: 0.7435 - accuracy: 0.7119 - val_loss: 1.1740 - val_accuracy: 0.5169\n",
      "Epoch 4/6\n",
      "19795/19795 [==============================] - 134s 7ms/step - loss: 0.6386 - accuracy: 0.7605 - val_loss: 0.9709 - val_accuracy: 0.6251\n",
      "Epoch 5/6\n",
      "19795/19795 [==============================] - 136s 7ms/step - loss: 0.5550 - accuracy: 0.7892 - val_loss: 1.1519 - val_accuracy: 0.5985\n",
      "Epoch 6/6\n",
      "19795/19795 [==============================] - 135s 7ms/step - loss: 0.4822 - accuracy: 0.8190 - val_loss: 1.1311 - val_accuracy: 0.6216\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=6, batch_size=batch_size, verbose=1, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2593, 136) (2593, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\harsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# load model from local disk\n",
    "\n",
    "model = joblib.load('model_combinedata_83.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4626077514435577, 0.8391298055648804]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss and accuracy score\n",
    "\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_combinedata_83.pkl']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle file on disk \n",
    "joblib.dump(model, 'model_combinedata_83.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
